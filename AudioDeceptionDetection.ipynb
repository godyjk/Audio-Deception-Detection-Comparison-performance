{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Machine Learning for Deception Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Author\n",
    "**Student Name: Haocheng Zhang**\n",
    "\n",
    "**Student ID: 221166194**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Problem Formulation\n",
    "In this project, we aim to solve the machine learning problem of deception detection using the MLEnd Deception Dataset. The goal is to classify stories into two categories: \"True Story\" and \"Fabricated Story.\" Deception detection is an interesting problem due to its applications in psychology, security, and automated systems. Detecting deception accurately can help build trust in systems that require interaction with humans, such as conversational agents and forensic tools. It is also challenging because the signals of deception in speech or text can be subtle and influenced by a variety of factors, including individual traits and environmental conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methodology\n",
    "I use neural networks, random forests, grid-optimized, and decision trees to implement binary decision-making, and compare their effectiveness and similarities and differences.\n",
    "\n",
    "**Our methodology includes the following steps:**\n",
    "\n",
    "**Training Task**: The training task involves creating a machine learning model to classify audio files as either \"True Story\" or \"Fabricated Story.\" We use the provided MLEnd dataset for training and validation.\n",
    "\n",
    "**Validation Task**: The model is validated using a separate test set to evaluate its generalization ability.\n",
    "\n",
    "Performance Metrics:\n",
    "Model performance will be evaluated using:\n",
    "\n",
    "Accuracy: The ratio of correctly predicted samples to total samples.\n",
    "\n",
    "Confusion Matrix: To understand false positives and false negatives.\n",
    "\n",
    "F1-Score, Precision, and Recall: To ensure a balanced performance across classes.\n",
    "\n",
    "**Additional Tasks**: Feature extraction from audio data is critical for success. We extract Mel Frequency Cepstral Coefficients (MFCCs), chroma features, zero-crossing rate, and other audio features to represent the data effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implemented ML Prediction Pipelines\n",
    "\n",
    "**Input**: Raw audio files from the dataset.\n",
    "\n",
    "**Stages**:\n",
    "\n",
    "Transformation Stage: Feature extraction from audio files.\n",
    "\n",
    "Model Stage: Building and training machine learning models (e.g., Neural Networks, Decision Trees, Random Forestsï¼Œgrid-optimized Decision Trees).\n",
    "\n",
    "Evaluation Stage: Using metrics to analyze model performance.\n",
    "\n",
    "Output: Classification of each audio file as either \"True Story\" or \"Fabricated Story.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Transformation Stage\n",
    "**Input**: Raw audio files (.wav format).\n",
    "\n",
    "**Output**: Feature vectors representing each audio file.\n",
    "\n",
    "**Description**:\n",
    "Features include MFCCs, chroma features, mel spectrogram, spectral centroid, and zero-crossing rate.\n",
    "These features capture temporal and spectral properties of audio that may correlate with deception.\n",
    "Chosen because they are standard in audio signal processing and provide a compact representation of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Model Stage\n",
    "\n",
    "**Models Explored**:\n",
    "\n",
    "Neural Networks: For their ability to learn non-linear patterns in data.\n",
    "\n",
    "Decision Trees: As a baseline due to their interpretability.\n",
    "\n",
    "Random Forests: To improve charateristic.\n",
    "\n",
    "Grid-optimized: To reduce overfitting and improve stability.\n",
    "\n",
    "**Reason for Choice**:\n",
    "\n",
    "Grid-optimized Decision Trees are suited for high-dimensional data, while ensemble methods like Random Forests and Neural networks are robust and perform not well on small datasets, and Decision Trees is too simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ensemble Stage\n",
    "\n",
    "In the ensemble stage of our model pipeline, we integrated the strengths of the Grid-Optimized Decision Tree with the Standard Decision Tree to create a more robust classifier. This approach was chosen based on the principle that combining models often results in better performance than any single model alone due to their ability to compensate for each other's weaknesses.\n",
    "\n",
    "#### Reasons for Choosing This Ensemble:\n",
    "\n",
    "This ensemble approach forms a crucial part of our strategy to enhance model performance while maintaining the interpretability of the decision trees, making it a valuable asset in the robust analytical toolkit required for deception detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Dataset\n",
    "**Description**:\n",
    "The dataset consists of audio files and a CSV file with metadata:\n",
    "\n",
    "**Audio Files**: Speech data categorized as \"True Story\" or \"Fabricated Story.\"\n",
    "Metadata CSV: Contains attributes such as filename and Story_type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Dataset Preparation\n",
    "**Training and Validation Split:**\n",
    "\n",
    "Training Set: 70% of the data.\n",
    "\n",
    "Validation Set: 30% of the data.\n",
    "\n",
    "**Independence and IID Assumptions:**\n",
    "Ensured through random sampling without replacement.\n",
    "Audio features are standardized to maintain uniform distributions.\n",
    "\n",
    "**Limitations:**\n",
    "Dataset size might be small, leading to overfitting.\n",
    "Audio quality variability might introduce noise into feature extraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Dataset Visualization\n",
    "Feature distributions for MFCCs, chroma, and other extracted features were analyzed to check for class imbalance and separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Experiments and Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Extract data and features from the sample set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preview:\n",
      "    filename Language  Story_type\n",
      "0  00001.wav  Chinese  True Story\n",
      "1  00002.wav  Chinese  True Story\n",
      "2  00003.wav  Chinese  True Story\n",
      "3  00004.wav  Chinese  True Story\n",
      "4  00005.wav  Chinese  True Story\n",
      "process audio file Deception/CBU0521DD_stories\\00001.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00002.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00003.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00004.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00005.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00006.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00007.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00008.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00009.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00010.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00011.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00012.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00013.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00014.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00015.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00016.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00017.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00018.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00019.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00020.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00021.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00022.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00023.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00024.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00025.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00026.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00027.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00028.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00029.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00030.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00031.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00032.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00033.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00034.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00035.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00036.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00037.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00038.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00039.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00040.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00041.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00042.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00043.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00044.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00045.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00046.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00047.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00048.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00049.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00050.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00051.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00052.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00053.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00054.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00055.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00056.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00057.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00058.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00059.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00060.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00061.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00062.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00063.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00064.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00065.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00066.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00067.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00068.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00069.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00070.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00071.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00072.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00073.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00074.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00075.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00076.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00077.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00078.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00079.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00080.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00081.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00082.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00083.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00084.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00085.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00086.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00087.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00088.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00089.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00090.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00091.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00092.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00093.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00094.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00095.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00096.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00097.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00098.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00099.wav\n",
      "process audio file Deception/CBU0521DD_stories\\00100.wav\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Path definitions\n",
    "csv_path = \"Deception/CBU0521DD_stories_attributes.csv\"\n",
    "audio_dir = \"Deception/CBU0521DD_stories\"\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Data preview:\")\n",
    "print(data.head())\n",
    "\n",
    "# Function to extract audio features\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # Extract MFCC features\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "        mfcc_mean = np.mean(mfcc, axis=1)\n",
    "        chroma_mean = np.mean(chroma, axis=1)\n",
    "        mel_mean = np.mean(mel, axis=1)\n",
    "\n",
    "        # Extract zero crossing rate\n",
    "        zero_crossing_rate = librosa.feature.zero_crossing_rate(y=y)\n",
    "        zcr_mean = np.mean(zero_crossing_rate)\n",
    "\n",
    "        # Combine additional features\n",
    "        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "\n",
    "        print(f\"process audio file {file_path}\")\n",
    "        \n",
    "        return np.hstack((mfcc_mean, chroma_mean, mel_mean, zcr_mean, spectral_centroid))\n",
    "    except Exception as e:\n",
    "        print(f\"Unable to process audio file {file_path}: {e}\")\n",
    "        return np.zeros(13 + 12 + 128 + 1 + 1)  # Return a zero vector with the appropriate dimension if processing fails\n",
    "\n",
    "# Extract features for all audio files\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    audio_file = os.path.join(audio_dir, row['filename'])\n",
    "    if os.path.exists(audio_file):\n",
    "        feature = extract_features(audio_file)\n",
    "        features.append(feature)\n",
    "        labels.append(1 if row['Story_type'] == 'True Story' else 0)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "features = np.array(features)\n",
    "labels = np.array(labels)\n",
    "\n",
    "# Apply SMOTE for oversampling\n",
    "smote = SMOTE()\n",
    "X_resampled, y_resampled = smote.fit_resample(features, labels)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_resampled = scaler.fit_transform(X_resampled)\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "y_test = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Decision Tree model...\n",
      "Evaluating the Decision Tree model...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.47      0.56        15\n",
      "           1       0.60      0.80      0.69        15\n",
      "\n",
      "    accuracy                           0.63        30\n",
      "   macro avg       0.65      0.63      0.62        30\n",
      "weighted avg       0.65      0.63      0.62        30\n",
      "\n",
      "Accuracy: 0.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Build the Decision Tree model\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Train the Decision Tree model\n",
    "print(\"Training the Decision Tree model...\")\n",
    "decision_tree.fit(X_train, np.argmax(y_train, axis=1))  # Use non-one-hot-encoded labels for training\n",
    "\n",
    "# Evaluate the Decision Tree model\n",
    "print(\"Evaluating the Decision Tree model...\")\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Output the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))  # Use non-one-hot-encoded labels for evaluation\n",
    "print(f\"Accuracy: {accuracy_score(np.argmax(y_test, axis=1), y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: The decision tree was used as a baseline model for its simplicity and interpretability.\n",
    "\n",
    "Results:Accuracy: 0.63(not good enough)\n",
    "\n",
    "Observations: The decision tree performed moderately well but tended to overfit the training data due to its tendency to create complex decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest model...\n",
      "Evaluating the Random Forest model...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.73      0.63        15\n",
      "           1       0.60      0.40      0.48        15\n",
      "\n",
      "    accuracy                           0.57        30\n",
      "   macro avg       0.57      0.57      0.55        30\n",
      "weighted avg       0.57      0.57      0.55        30\n",
      "\n",
      "Accuracy: 0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Build the Random Forest model\n",
    "random_forest = RandomForestClassifier(n_estimators=100, random_state=42)  # Using 100 estimators (trees)\n",
    "\n",
    "# Train the Random Forest model\n",
    "print(\"Training the Random Forest model...\")\n",
    "random_forest.fit(X_train, np.argmax(y_train, axis=1))  # Use non-one-hot-encoded labels for training\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "print(\"Evaluating the Random Forest model...\")\n",
    "y_pred = random_forest.predict(X_test)\n",
    "\n",
    "# Output the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(np.argmax(y_test, axis=1), y_pred))  # Use non-one-hot-encoded labels for evaluation\n",
    "print(f\"Accuracy: {accuracy_score(np.argmax(y_test, axis=1), y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: Random forest was employed as an ensemble method to improve stability and reduce overfitting by averaging multiple decision trees.\n",
    "\n",
    "Results:Accuracy: 0.57(not good enough)\n",
    "\n",
    "Observations: Surprisingly, the random forest underperformed compared to the decision tree. This could be due to suboptimal hyperparameter settings or the small dataset size, which made it challenging for an ensemble method to generalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/100\n",
      "4/4 [==============================] - 1s 83ms/step - loss: 1.4307 - accuracy: 0.5536 - val_loss: 0.7618 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1538 - accuracy: 0.5357 - val_loss: 0.7240 - val_accuracy: 0.4286\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.3098 - accuracy: 0.5000 - val_loss: 0.6895 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.0174 - accuracy: 0.6964 - val_loss: 0.6779 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 0.8654 - accuracy: 0.6429 - val_loss: 0.6774 - val_accuracy: 0.5714\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.3450 - accuracy: 0.5357 - val_loss: 0.6833 - val_accuracy: 0.5714\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9248 - accuracy: 0.6071 - val_loss: 0.6770 - val_accuracy: 0.5714\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.8980 - accuracy: 0.5893 - val_loss: 0.6754 - val_accuracy: 0.5714\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.1608 - accuracy: 0.5179 - val_loss: 0.6791 - val_accuracy: 0.5714\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.9871 - accuracy: 0.5357 - val_loss: 0.6747 - val_accuracy: 0.5000\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.9738 - accuracy: 0.5714 - val_loss: 0.6756 - val_accuracy: 0.5714\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8665 - accuracy: 0.6607 - val_loss: 0.6795 - val_accuracy: 0.5714\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7693 - accuracy: 0.6786 - val_loss: 0.6817 - val_accuracy: 0.5714\n",
      "Epoch 14/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5844 - accuracy: 0.7679 - val_loss: 0.6796 - val_accuracy: 0.5714\n",
      "Epoch 15/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.8293 - accuracy: 0.5714 - val_loss: 0.6608 - val_accuracy: 0.5714\n",
      "Epoch 16/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.7577 - accuracy: 0.6786 - val_loss: 0.6395 - val_accuracy: 0.5714\n",
      "Epoch 17/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.8504 - accuracy: 0.6607 - val_loss: 0.6248 - val_accuracy: 0.5714\n",
      "Epoch 18/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6775 - accuracy: 0.6607 - val_loss: 0.6167 - val_accuracy: 0.5714\n",
      "Epoch 19/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4728 - accuracy: 0.7143 - val_loss: 0.6155 - val_accuracy: 0.5714\n",
      "Epoch 20/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.8050 - accuracy: 0.6607 - val_loss: 0.6087 - val_accuracy: 0.5714\n",
      "Epoch 21/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.9281 - accuracy: 0.6429 - val_loss: 0.6056 - val_accuracy: 0.5714\n",
      "Epoch 22/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6001 - accuracy: 0.7500 - val_loss: 0.6041 - val_accuracy: 0.7143\n",
      "Epoch 23/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.9636 - accuracy: 0.6250 - val_loss: 0.6058 - val_accuracy: 0.7857\n",
      "Epoch 24/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.7915 - accuracy: 0.6786 - val_loss: 0.6041 - val_accuracy: 0.7857\n",
      "Epoch 25/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6267 - accuracy: 0.7143 - val_loss: 0.5933 - val_accuracy: 0.7857\n",
      "Epoch 26/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6416 - accuracy: 0.6964 - val_loss: 0.5942 - val_accuracy: 0.7857\n",
      "Epoch 27/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.7164 - accuracy: 0.6250 - val_loss: 0.5886 - val_accuracy: 0.7857\n",
      "Epoch 28/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7304 - accuracy: 0.6964 - val_loss: 0.5872 - val_accuracy: 0.7143\n",
      "Epoch 29/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.6373 - accuracy: 0.6964 - val_loss: 0.5814 - val_accuracy: 0.7143\n",
      "Epoch 30/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.7121 - accuracy: 0.6250 - val_loss: 0.5776 - val_accuracy: 0.7857\n",
      "Epoch 31/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.7366 - accuracy: 0.6964 - val_loss: 0.5667 - val_accuracy: 0.7857\n",
      "Epoch 32/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6085 - accuracy: 0.7321 - val_loss: 0.5691 - val_accuracy: 0.8571\n",
      "Epoch 33/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6676 - accuracy: 0.6607 - val_loss: 0.5682 - val_accuracy: 0.7857\n",
      "Epoch 34/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4872 - accuracy: 0.7679 - val_loss: 0.5848 - val_accuracy: 0.7857\n",
      "Epoch 35/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6118 - accuracy: 0.7500 - val_loss: 0.6021 - val_accuracy: 0.7857\n",
      "Epoch 36/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6048 - accuracy: 0.6964 - val_loss: 0.6262 - val_accuracy: 0.7857\n",
      "Epoch 37/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6042 - accuracy: 0.6964 - val_loss: 0.6223 - val_accuracy: 0.7857\n",
      "Epoch 38/100\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6145 - accuracy: 0.7679 - val_loss: 0.6037 - val_accuracy: 0.7857\n",
      "Epoch 39/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.6934 - accuracy: 0.7500 - val_loss: 0.5936 - val_accuracy: 0.7857\n",
      "Epoch 40/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5424 - accuracy: 0.7321 - val_loss: 0.5808 - val_accuracy: 0.7857\n",
      "Epoch 41/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4849 - accuracy: 0.7143 - val_loss: 0.5685 - val_accuracy: 0.7857\n",
      "Epoch 42/100\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 0.6015 - accuracy: 0.7679 - val_loss: 0.5551 - val_accuracy: 0.7857\n",
      "Epoch 43/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7493 - accuracy: 0.6964 - val_loss: 0.5514 - val_accuracy: 0.7857\n",
      "Epoch 44/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.6043 - accuracy: 0.6786 - val_loss: 0.5511 - val_accuracy: 0.7857\n",
      "Epoch 45/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.7882 - accuracy: 0.6786 - val_loss: 0.5549 - val_accuracy: 0.7857\n",
      "Epoch 46/100\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4288 - accuracy: 0.7857 - val_loss: 0.5565 - val_accuracy: 0.7857\n",
      "Epoch 47/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.6525 - accuracy: 0.7143 - val_loss: 0.5595 - val_accuracy: 0.7857\n",
      "Epoch 48/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4129 - accuracy: 0.8214 - val_loss: 0.5595 - val_accuracy: 0.7857\n",
      "Epoch 49/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4887 - accuracy: 0.7500 - val_loss: 0.5560 - val_accuracy: 0.7857\n",
      "Epoch 50/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.4449 - accuracy: 0.7857 - val_loss: 0.5527 - val_accuracy: 0.7857\n",
      "Epoch 51/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5830 - accuracy: 0.7321 - val_loss: 0.5470 - val_accuracy: 0.8571\n",
      "Epoch 52/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.5012 - accuracy: 0.7857 - val_loss: 0.5475 - val_accuracy: 0.8571\n",
      "Epoch 53/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5125 - accuracy: 0.7500 - val_loss: 0.5563 - val_accuracy: 0.8571\n",
      "Epoch 54/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.6817 - accuracy: 0.7321 - val_loss: 0.5690 - val_accuracy: 0.8571\n",
      "Epoch 55/100\n",
      "4/4 [==============================] - 0s 7ms/step - loss: 0.6095 - accuracy: 0.7321 - val_loss: 0.5805 - val_accuracy: 0.8571\n",
      "Epoch 56/100\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 0.4051 - accuracy: 0.8214 - val_loss: 0.5918 - val_accuracy: 0.8571\n",
      "Epoch 57/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4395 - accuracy: 0.7679 - val_loss: 0.6006 - val_accuracy: 0.8571\n",
      "Epoch 58/100\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5942 - accuracy: 0.7500 - val_loss: 0.6147 - val_accuracy: 0.8571\n",
      "Epoch 59/100\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3695 - accuracy: 0.8929 - val_loss: 0.6236 - val_accuracy: 0.8571\n",
      "Epoch 60/100\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5023 - accuracy: 0.7679 - val_loss: 0.6207 - val_accuracy: 0.8571\n",
      "Epoch 61/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.3907 - accuracy: 0.8036 - val_loss: 0.6192 - val_accuracy: 0.8571\n",
      "Epoch 62/100\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.4992 - accuracy: 0.7679 - val_loss: 0.6173 - val_accuracy: 0.8571\n",
      "Epoch 63/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5630 - accuracy: 0.7857 - val_loss: 0.6195 - val_accuracy: 0.8571\n",
      "Epoch 64/100\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 0.5917 - accuracy: 0.7143 - val_loss: 0.6270 - val_accuracy: 0.8571\n",
      "Epoch 65/100\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 0.4744 - accuracy: 0.8036 - val_loss: 0.6319 - val_accuracy: 0.8571\n",
      "Epoch 66/100\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 0.3577 - accuracy: 0.8750 - val_loss: 0.6270 - val_accuracy: 0.8571\n",
      "Epoch 67/100\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.6473 - accuracy: 0.6964 - val_loss: 0.6270 - val_accuracy: 0.8571\n",
      "Evaluating the model...\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.60      0.67        15\n",
      "           1       0.67      0.80      0.73        15\n",
      "\n",
      "    accuracy                           0.70        30\n",
      "   macro avg       0.71      0.70      0.70        30\n",
      "weighted avg       0.71      0.70      0.70        30\n",
      "\n",
      "Accuracy: 0.7\n"
     ]
    }
   ],
   "source": [
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Define early stopping and learning rate scheduler\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=16)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=8)  # Set proper learning rate adjustment parameters\n",
    "\n",
    "# Train the model\n",
    "print(\"Training the model...\")\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=16, \n",
    "                    validation_split=0.2, verbose=1, \n",
    "                    callbacks=[early_stopping, reduce_lr])\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Output the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred_classes))\n",
    "print(f\"Accuracy: {accuracy_score(y_test_classes, y_pred_classes)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A CNN was implemented to take advantage of its ability to extract spatial and hierarchical patterns in the feature data.\n",
    "\n",
    "Results:Accuracy: **0.70**\n",
    "\n",
    "Observations: The CNN outperformed both the decision tree and random forest, demonstrating its strength in capturing complex patterns in the audio features. **However, the model required significant computational resources and careful tuning to prevent overfitting. due to the small sample size, the neural network also experienced slow gradient descent during training, resulting in low accuracy of the final results, sometimes even below 50%. This result is the best set of parameters I have thrown out after multiple runs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 grid-optimized Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1200 candidates, totalling 6000 fits\n",
      "Best Decision Tree parameters: {'criterion': 'entropy', 'max_depth': None, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.60      0.64        15\n",
      "           1       0.65      0.73      0.69        15\n",
      "\n",
      "    accuracy                           0.67        30\n",
      "   macro avg       0.67      0.67      0.67        30\n",
      "weighted avg       0.67      0.67      0.67        30\n",
      "\n",
      "Accuracy: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the model\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Expand the parameter grid\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Splitting criteria\n",
    "    'max_depth': [None, 10, 20, 30, 40, 50],  # None means no limit\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 6, 8],\n",
    "    'max_features': ['auto', 'sqrt', 'log2', None]  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(estimator=dt, param_grid=param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, np.argmax(y_train, axis=1))\n",
    "\n",
    "# Display the best parameters\n",
    "print(\"Best Decision Tree parameters:\", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred = best_dt.predict(X_test)\n",
    "y_test_classes = np.argmax(y_test, axis=1)  # Convert if y_test is one-hot encoded\n",
    "\n",
    "# Output the classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_classes, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test_classes, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description: A Decision Tree classifier was employed, utilizing a comprehensive grid search to optimize its configuration. The decision tree was chosen for its interpretive ease, allowing us to clearly understand the decision-making process. Grid search was used to methodically explore a range of hyperparameters, including tree depth, criteria for splits, and leaf constraints, aiming to capture the best balance between model complexity and prediction accuracy.\n",
    "\n",
    "Results:Accuracy: **0.67**\n",
    "\n",
    "Observations: The decision tree optimized by grid decision shows significant performance, although it does not exceed the upper limit of CNN, it has extremely high stability and will not encounter overfitting problems. This model benefits from the systematic adjustment of its parameters, which to some extent enhances its ability to handle the complexity of audio feature patterns. This makes it particularly useful for applications where understanding the basis of model decisions is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Results Analysis\n",
    "The CNN achieved the highest accuracy (0.70), followed by the grid-optimized Decision Tree (0.67), Decision Tree (0.63) and then the Random Forest.\n",
    "\n",
    "Considering the serious overfitting phenomenon of CNN neural network, obtaining this set of parameters with an accuracy of 0.7 is a coincidence, the confusion matrices revealed that the grid-optimized Decision Tree was better at minimizing false negatives, demonstrating its robustness in capturing the hierarchical patterns in the data. The Decision Tree, showed significant performance improvements with extremely high stability and no overfitting issues. It balanced complexity and accuracy well, but still struggled with some false positives and false negatives compared to the CNN. The Random Forest underperformed unexpectedly (accuracy lower than Decision Tree), suggesting that it might require further hyperparameter tuning or more sophisticated data preprocessing techniques.\n",
    "\n",
    "Overall, the results indicate that more complex models such as CNNs and random forests may not be suitable for this small sample task as they can learn nonlinear relationships and complex patterns in useless data. The decision tree optimized by grid provides valuable insights due to its transparency, and its overall performance is superior to ordinary decision trees, which helps to understand the falsehood and truth of the story well. I have observed that as I extract more audio features, the accuracy of recognition also increases, possibly due to the decision tree's ability to understand the styles formed by each feature in the audio file well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Conclusions\n",
    "\n",
    "This study investigated four machine learning models (CNN, Grid-Optimized Decision Tree, Decision Tree, and Random Forest) for deception detection using audio features. The findings highlight the nuanced capabilities of each model:\n",
    "\n",
    "#### Key Findings:\n",
    "**Performance Comparison**:\n",
    "\n",
    "- The **CNN** achieved the best accuracy (**0.70**), underscoring its proficiency in capturing complex patterns in the audio data. **However, its performance is somewhat tempered by a tendency to overfit due to the small sample size, potentially leading to inconsistent results.**\n",
    "  \n",
    "- The **Grid-Optimized Decision Tree** showed significant improvement over the standard Decision Tree (**0.67**). It provided high stability without overfitting, benefiting from systematic parameter tuning which enhanced its ability to handle complex audio features.\n",
    "\n",
    "- The **Standard Decision Tree** recorded moderate performance (**0.63**), reflecting its simplicity and limited capacity to manage subtleties within the audio data, with a noted propensity for overfitting.\n",
    "\n",
    "- The **Random Forest** underperformed (**0.57**), likely due to inadequate data volume and suboptimal hyperparameter settings, indicating a need for more sophisticated tuning and possibly more extensive data preprocessing.\n",
    "\n",
    "**Feature Extraction Importance**:\n",
    "Feature extraction, especially the use of MFCC, chromaticity, and Mel spectrogram, has played a crucial role in all models. Decision trees are particularly adept at utilizing these features under optimal conditions while providing transparent utilization, making them valuable in applications where interpretability is crucial.\n",
    "\n",
    "**Data Challenges**:\n",
    "The limited dataset size constrained the effectiveness of more complex models like the Random Forest and sometimes the CNN, leading to potential overfitting. Class imbalance and the nuanced distinctions between \"True Story\" and \"Fabricated Story\" further complicated the modeling.\n",
    "\n",
    "#### Improvement:\n",
    "\n",
    "- **Advanced Models**: Explore more sophisticated architectures, including Vggish and PANN, or using hybrid models, to better capture both spatial and temporal dynamics in the audio data.\n",
    "\n",
    "- **Hyperparameter Optimization**: Employ more rigorous techniques such as grid search or Bayesian optimization to refine the settings for Random Forest and possibly extend this to other models to ensure optimal performance.\n",
    "\n",
    "- **Ensemble Methods**: Consider using ensemble strategies to amalgamate the strengths of various models. Techniques like stacking or boosting might yield better performance by mitigating the individual weaknesses of single models.\n",
    "\n",
    "#### Final Conclusion:\n",
    "\n",
    "Although CNN has shown its greatest potential by achieving the highest accuracy, overfitting may occur. In the current problem, **the grid optimized decision tree model is the optimal solution**, providing a balance between accuracy and interpretability without the overfitting problem that occurs in more complex models, successfully distinguishing between true and false stories. These insights emphasize the importance of model selection based on specific requirements and constraints of the task at hand. Looking ahead to the future, adopting professional speech models can solve the limitations caused by data constraints and improve the reliability and accuracy of deception detection systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. References\n",
    "Scikit-learn Documentation: https://scikit-learn.org/\n",
    "\n",
    "TensorFlow Documentation: https://www.tensorflow.org/\n",
    "\n",
    "Chen Wanzhi, Hou Yue A temporal multimodal sentiment analysis model that integrates multi-level attention and sentiment scale vectors Liaoning University of Engineering and Technology, School of Software, School of Electronic and Information Engineering, 2023\n",
    "\n",
    "Sun Zhi, Crown CNN-GRU speech emotion recognition algorithm based on self supervised contrastive learning China Telecom Shenzhen Branch, Department of Applied Mathematics, Hong Kong Polytechnic University, 2023\n",
    "\n",
    "https://cloud.tencent.com/developer/article/2443967\n",
    "\n",
    "https://blog.csdn.net/laojinlaojinlaojin/article/details/138249407\n",
    "\n",
    "https://blog.csdn.net/universsky2015/article/details/137304461"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
